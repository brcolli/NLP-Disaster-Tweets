{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kaggle_NLP_Disaster_Tweets.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LX66rx3ipEmi","colab_type":"code","colab":{}},"source":["import numpy as np   # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os            # Handle file paths\n","import string        # String functionality\n","import re            # Regex\n","import nltk          # Corpus for stopwords, lemmatizing, etc.\n","\n","# Input data files are available in the \"../input/\" directory.\n","# Any results you write to the current directory are saved as output.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# Hey Max\n","\n","class DisasterTweets:\n","    \n","    def __init__(self):\n","        \n","        for dirname, _, filenames in os.walk('/kaggle/input'):\n","            # Save CSVs\n","            if len(filenames):\n","                self.train_set  = pd.read_csv(os.path.join(dirname, filenames[0]))\n","                self.test_set   = pd.read_csv(os.path.join(dirname, filenames[1]))\n","    \n","    \n","    def has_empty_data(self):\n","        return not (len(self.train_set) and len(self.test_set))  # Returns true if any dataset is empty\n","                \n","\n","# Discards all punctuation as classified by string.punctuation\n","def remove_punctuation(text):\n","    return \"\".join([char for char in text if char not in string.punctuation])\n","\n","\n","# Separates text into an array of tokenized strings\n","def tokenize(text):\n","    return re.split('\\W+', text)  # Could add another input to specifiy a parse string\n","\n","\n","# Removes Enlgish stopwords from a tokenized list\n","def remove_stopwords(token_text):\n","    stopword = nltk.corpus.stopwords.words('english')  # All English stopwords\n","    return [word for word in token_text if word not in stopword]\n","\n","\n","# Stems a tokenized list\n","def stem_tokens(token_text):\n","    ps = nltk.PorterStemmer()\n","    return [ps.stem(word) for word in token_text]\n","\n","\n","#  Lemmatize a tokenized list\n","def lemmatize_tokens(token_text):\n","    wn = nltk.WordNetLemmatizer()\n","    return [wn.lemmatize(word) for word in token_text]\n","    \n","    \n","def main():\n","    \n","    disaster_tweets = DisasterTweets()\n","    \n","    if disaster_tweets.has_empty_data():\n","        return  # Failed to read input data\n","    \n","    print(disaster_tweets.train_set)\n","    print(disaster_tweets.test_set)\n","    # hello 12\n","\n","    ####################################\n","    # Possible data sanitization efforts\n","    ####################################\n","    \n","    # Remove URLs\n","    \n","    # Remove hashtag symbols, tag hashtag words\n","    \n","    # Bin locations\n","    \n","    # Tokenize\n","    \n","    # Remove stop words\n","    # Removing words that don't give us valuable information, such as 'the, a, of'\n","    \n","    # Lemmatizing\n","    # A more extreme case of stemming. Uses dictionary look ups to find the proper root word.\n","    # For example, 'Entitling' would give us 'Entitl' from stemming, but lemmatizing would\n","    # give us 'Entitle'.\n","    \n","    # Use N-grams to find relationships between words in sentences\n","    \n","    # Use Support Vector Machine?\n","    \n","    # After training model, we need to do a regularization\n","\n","\n","main()\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtzs5V3TrP8y","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]}]}