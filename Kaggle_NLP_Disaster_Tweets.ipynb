{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benrc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benrc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "#         IMPORT LIBRARIES         #\n",
    "####################################\n",
    "\n",
    "import numpy as np      # linear algebra\n",
    "import pandas as pd     # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os               # Handle file paths\n",
    "import string           # String functionality\n",
    "import re               # Regex\n",
    "import nltk             # Corpus for stopwords, lemmatizing, etc.\n",
    "import shapefile as shp # Reading shape files for geography\n",
    "\n",
    "# Sklearn TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#              CLASSES             #\n",
    "####################################\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# Any results you write to the current directory are saved as output.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory.\n",
    "\n",
    "class DisasterTweets:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "            # Save CSVs\n",
    "            if len(filenames):\n",
    "                self.train_set  = pd.read_csv(os.path.join(dirname, filenames[0]))\n",
    "                self.test_set   = pd.read_csv(os.path.join(dirname, filenames[1]))\n",
    "\n",
    "\n",
    "    def __init__(self, trainFile, testFile):\n",
    "        self.train_set  = pd.read_csv(trainFile)\n",
    "        self.test_set   = pd.read_csv(testFile)\n",
    "\n",
    "\n",
    "    def has_empty_data(self):\n",
    "        return not (len(self.train_set) and len(self.test_set))  # Returns true if any dataset is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#         HELPER FUNCTIONS         #\n",
    "####################################\n",
    "\n",
    "# Discards all punctuation as classified by string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "\n",
    "# A URL is defined as a string that begins with http or https followed by\n",
    "# :// and then any set of characters up to a whitespace\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http?:\\/\\/\\S+\\s?', '', text)\n",
    "\n",
    "\n",
    "# Remove non-standard ASCII characters below char value 128\n",
    "def remove_bad_ascii(text):\n",
    "    return \"\".join(i for i in text if ord(i)<128)\n",
    "\n",
    "\n",
    "# Removing numbers from the text, might not actually be a good idea.\n",
    "# Take this into consideration\n",
    "def remove_numbers(text):\n",
    "    return \"\".join([char for char in text if not char.isdigit()])\n",
    "\n",
    "\n",
    "def collect_hashtags(text):\n",
    "    return re.findall(r'#(\\S*)\\s?', text)\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#\\S*\\s?', '', text)\n",
    "\n",
    "\n",
    "# Run the various sanitization functions\n",
    "def sanitize_text(text):\n",
    "\n",
    "    clean_text = remove_punctuation(text)\n",
    "    clean_text = remove_urls(clean_text)\n",
    "    clean_text = remove_bad_ascii(clean_text)\n",
    "    clean_text = remove_hashtags(clean_text)\n",
    "    clean_text = remove_numbers(clean_text)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def collect_locations(countries_shp, spl_shp):\n",
    "\n",
    "    c_shp = shp.Reader(countries_shp)\n",
    "    s_shp = shp.Reader(spl_shp)\n",
    "\n",
    "    c_list = [row[3] for row in c_shp.records()] # Countries\n",
    "    s_list = [row[8] for row in s_shp.records()] # States\n",
    "    a_list = [row[9][:2] for row in s_shp.records()] # State acronyms\n",
    "\n",
    "    return c_list + s_list + a_list # Compile to one big list\n",
    "\n",
    "\n",
    "# Collect all the locations stated and cross reference in the shape file,\n",
    "# looking for real-life places\n",
    "def bin_locations(location, location_set):\n",
    "\n",
    "    if type(location) == str and location: # Ensure it is a string, AKA not NaN\n",
    "\n",
    "        # Ensure the string contains alphabetical characters of at least length 2\n",
    "        match = re.search(\"[a-zA-Z]{2,}\", location)\n",
    "        if match:\n",
    "            location_split = ' '.join(re.split(',|\\s', location.lower())).split() # Split on commas and whitespace\n",
    "\n",
    "            for l in location_set:\n",
    "                if l.lower() in location_split:\n",
    "                    return location # Return the base location, as it appears real\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "# Separates text into an array of tokenized strings\n",
    "def tokenize(text):\n",
    "    return re.split('\\W+', text)  # Could add another input to specifiy a parse string\n",
    "\n",
    "\n",
    "# Removes Enlgish stopwords from a tokenized list\n",
    "def remove_stopwords(token_text):\n",
    "    stopword = nltk.corpus.stopwords.words('english')  # All English stopwords\n",
    "    return [word for word in token_text if word not in stopword and word]\n",
    "\n",
    "\n",
    "#  Lemmatize a tokenized list\n",
    "def lemmatize_tokens(token_text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    return [wn.lemmatize(word) for word in token_text]\n",
    "\n",
    "\n",
    "def compute_tf_idf(tweets):\n",
    "\n",
    "    # Dummy function to trick sklearn into taking token list\n",
    "    def dummy_func(doc):\n",
    "        return doc\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                analyzer='word',\n",
    "                tokenizer=dummy_func,\n",
    "                preprocessor=dummy_func,\n",
    "                token_pattern=None\n",
    "    )\n",
    "\n",
    "    vectors = vectorizer.fit_transform(tweets)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "\n",
    "    # Map TF-IDF results to dictionary\n",
    "    tf_idf_list = []\n",
    "    for tweetList in denselist:\n",
    "        tf_idf_dict = dict.fromkeys(feature_names, 0)\n",
    "        for i in range(0, len(feature_names)):\n",
    "            tf_idf_dict[feature_names[i]] = tweetList[i]\n",
    "        tf_idf_list.append(tf_idf_dict)\n",
    "\n",
    "    return pd.Series(data=tf_idf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#          GENERATE DATA           #\n",
    "####################################\n",
    "disaster_tweets = DisasterTweets('data/train.csv', 'data/test.csv')\n",
    "\n",
    "if disaster_tweets.has_empty_data():\n",
    "    raise ValueError('Could not read data CSVs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#        PREPROCESSING DATA        #\n",
    "####################################\n",
    "\n",
    "# Collect hashtags before cleaning anything\n",
    "disaster_tweets.test_set['hashtags'] = disaster_tweets.test_set['text'].map(collect_hashtags)\n",
    "\n",
    "# Sanitize the data\n",
    "disaster_tweets.test_set['text'] = disaster_tweets.test_set['text'].map(sanitize_text)\n",
    "\n",
    "# Bin locations\n",
    "location_data = collect_locations('data/Countries/ne_110m_admin_0_countries.shp', 'data/States and Provinces/ne_110m_admin_1_states_provinces_lakes.shp')\n",
    "disaster_tweets.test_set['true_location'] = disaster_tweets.test_set['location'].apply(bin_locations, args=(location_data,))\n",
    "\n",
    "# Tokenize\n",
    "disaster_tweets.test_set['tokenized'] = disaster_tweets.test_set['text'].map(tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "# Removing words that don't give us valuable information, such as 'the, a, of'\n",
    "disaster_tweets.test_set['tokenized'] = disaster_tweets.test_set['tokenized'].map(remove_stopwords)\n",
    "\n",
    "# Lemmatizing\n",
    "# A more extreme case of stemming. Uses dictionary look ups to find the proper root word.\n",
    "# For example, 'Entitling' would give us 'Entitl' from stemming, but lemmatizing would\n",
    "# give us 'Entitle'.\n",
    "disaster_tweets.test_set['tokenized'] = disaster_tweets.test_set['tokenized'].map(lemmatize_tokens)\n",
    "\n",
    "# Generate term frequency-inverse data frequency (TF-IDF)\n",
    "disaster_tweets.test_set['tf_idf'] = compute_tf_idf(disaster_tweets.test_set['tokenized'])\n",
    "\n",
    "# Use N-grams to find relationships between words in sentences\n",
    "\n",
    "# Use Support Vector Machine?\n",
    "\n",
    "# After training model, we need to do a regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
